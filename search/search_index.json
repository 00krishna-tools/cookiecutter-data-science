{
    "docs": [
        {
            "location": "/",
            "text": "Cookiecutter Data Science\n\n\nA logical, reasonably standardized, but flexible project structure for doing and sharing data science work.\n\n\nWhy use this project structure?\n\n\n\n\nWe're not talking about bikeshedding the indentation aesthetics or pedantic formatting standards \u2014 ultimately, data science code quality is about correctness and reproducibility.\n\n\n\n\nWhen we think about data analysis, we often think just about the resulting reports, insights, or visualizations. While these end products are generally the main event, it's easy to focus on making the products \nlook nice\n and ignore the \nquality of the code that generates them\n. Because these end products are created programmatically, \ncode quality is still important\n! And we're not talking about bikeshedding the indentation aesthetics or pedantic formatting standards \u2014 ultimately, data science code quality is about correctness and reproducibility.\n\n\nIt's no secret that good analyses are often the result of very scattershot and serendipitous explorations. Tentative experiments and rapidly testing approaches that might not work out are all part of the process for getting to the good stuff, and there is no magic bullet to turn data exploration into a simple, linear progression.\n\n\nThat being said, once started it is not a process that lends itself to thinking carefully about the structure of your code or project layout, so it's best to start with a clean, logical structure and stick to it throughout. We think it's a pretty big win all around to use a fairly standardized setup like this one. Here's why:\n\n\nOther people will thank you\n\n\n\n\nNobody sits around before creating a new Rails project to figure out where they want to put their views; they just run \nrails new\n to get a standard project skeleton like everybody else.\n\n\n\n\nA well-defined, standard project structure means that a newcomer can begin to understand an analysis without digging in to extensive documentation. It also means that they don't necessarily have to read 100% of the code before knowing where to look for very specific things.\n\n\nWell organized code tends to be self-documenting in that the organization itself provides context for your code without much overhead. People will thank you for this because they can:\n\n\n\n\nCollaborate more easily with you on this analysis\n\n\nLearn from your analysis about the process and the domain\n\n\nFeel confident in the conclusions at which the analysis arrives\n\n\n\n\nA good example of this can be found in any of the major web development frameworks like Django or Ruby on Rails. Nobody sits around before creating a new Rails project to figure out where they want to put their views; they just run \nrails new\n to get a standard project skeleton like everybody else. Because that default project structure is \nlogical\n and \nreasonably standard across most projects\n, it is much easier for somebody who has never seen a particular project to figure out where they would find the various moving parts.\n\n\nAnother great example is the \nFilesystem Hierarchy Standard\n for Unix-like systems. The \n/etc\n directory has a very specific purpose, as does the \n/tmp\n folder, and everybody (more or less) agrees to honor that social contract. That means a Red Hat user and an Ubuntu user both know roughly where to look for certain types of files, even when using each other's system \u2014 or any other standards-compliant system for that matter!\n\n\nIdeally, that's how it should be when a colleague opens up your data science project.\n\n\nYou will thank you\n\n\nEver tried to reproduce an analysis that you did a few months ago or even a few years ago? You may have written the code, but it's now impossible to decipher whether you should use \nmake_figures.py.old\n, \nmake_figures_working.py\n or \nnew_make_figures01.py\n to get things done. Here are some questions we've learned to ask with a sense of existential dread:\n\n\n\n\nAre we supposed to go in and join the column X to the data before we get started or did that come from one of the notebooks?\n\n\nCome to think of it, which notebook do we have to run first before running the plotting code: was it \"process data\" or \"clean data\"?\n\n\nWhere did the shapefiles get downloaded from for the geographic plots?\n\n\nEt cetera, times infinity.\n\n\n\n\nThese types of questions are painful and are symptoms of a disorganized project. A good project structure encourages practices that make it easier to come back to old work, for example separation of concerns, abstracting analysis as a \nDAG\n, and engineering best practices like version control.\n\n\nNothing here is binding\n\n\n\n\n\"A foolish consistency is the hobgoblin of little minds\" \u2014 Ralph Waldo Emerson (and \nPEP 8!\n)\n\n\n\n\nDisagree with a couple of the default folder names? Working on a project that's a little nonstandard and doesn't exactly fit with the current structure? Prefer to use a different package than one of the (few) defaults?\n\n\nGo for it!\n This is a lightweight structure, and is intended to be a good \nstarting point\n for many projects. Or, as PEP 8 put it:\n\n\n\n\nConsistency within a project is more important. Consistency within one module or function is the most important. ... However, know when to be inconsistent -- sometimes style guide recommendations just aren't applicable. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask!\n\n\n\n\nGetting started\n\n\nWith this in mind, we've created a data science cookiecutter template for projects in Python. Your analysis doesn't have to be in Python, but the template does provide some Python boilerplate that you'd want to remove (in the \nsrc\n folder for example, and the Sphinx documentation skeleton in \ndocs\n).\n\n\nRequirements\n\n\n\n\nPython 2.7 or 3.5\n\n\ncookiecutter Python package\n >= 1.4.0: \npip install cookiecutter\n\n\n\n\nStarting a new project\n\n\nStarting a new project is as easy as running this command at the command line. No need to create a directory first, the cookiecutter will do it for you.\n\n\ncookiecutter https://github.com/drivendata/cookiecutter-data-science\n\n\n\n\nExample\n\n\n\n\n\nDirectory structure\n\n\n\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile           <- Makefile with commands like `make data` or `make train`\n\u251c\u2500\u2500 README.md          <- The top-level README for developers using this project.\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 external       <- Data from third party sources.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 interim        <- Intermediate data that has been transformed.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 processed      <- The final, canonical data sets for modeling.\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 raw            <- The original, immutable data dump.\n\u2502\n\u251c\u2500\u2500 docs               <- A default Sphinx project; see sphinx-doc.org for details\n\u2502\n\u251c\u2500\u2500 models             <- Trained and serialized models, model predictions, or model summaries\n\u2502\n\u251c\u2500\u2500 notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n\u2502                         the creator's initials, and a short `-` delimited description, e.g.\n\u2502                         `1.0-jqp-initial-data-exploration`.\n\u2502\n\u251c\u2500\u2500 references         <- Data dictionaries, manuals, and all other explanatory materials.\n\u2502\n\u251c\u2500\u2500 reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 figures        <- Generated graphics and figures to be used in reporting\n\u2502\n\u251c\u2500\u2500 requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n\u2502                         generated with `pip freeze > requirements.txt`\n\u2502\n\u251c\u2500\u2500 setup.py           <- Make this project pip installable with `pip install -e`\n\u251c\u2500\u2500 src                <- Source code for use in this project.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py    <- Makes src a Python module\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data           <- Scripts to download or generate data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 make_dataset.py\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 features       <- Scripts to turn raw data into features for modeling\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 build_features.py\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models         <- Scripts to train models and then use trained models to make\n\u2502   \u2502   \u2502                 predictions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 predict_model.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_model.py\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 visualization  <- Scripts to create exploratory and results oriented visualizations\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 visualize.py\n\u2502\n\u2514\u2500\u2500 tox.ini            <- tox file with settings for running tox; see tox.testrun.org\n\n\n\n\nOpinions\n\n\nThere are some opinions implicit in the project structure that have grown out of our experience with what works and what doesn't when collaborating on data science projects. Some of the opinions are about workflows, and some of the opinions are about tools that make life easier. Here are some of the beliefs which this project is built on\u2014if you've got thoughts, please \ncontribute or share them\n.\n\n\nData is immutable\n\n\nDon't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see \nAnalysis is a DAG\n), but anyone should be able to reproduce the final products with only the code in \nsrc\n and the data in \ndata/raw\n.\n\n\nAlso, if data is immutable, it doesn't need source control in the same way that code does. Therefore, \nby default, the data folder is included in the \n.gitignore\n file.\n If you have a small amount of data that rarely changes, you may want to include the data in the repository. Github currently warns if files are over 50MB and rejects files over 100MB. Some other options for storing/syncing large data include \nAWS S3\n with a syncing tool (e.g., \ns3cmd\n), \nGit Large File Storage\n, \nGit Annex\n, and \ndat\n. Currently by default, we ask for an S3 bucket and use \nAWS CLI\n to sync data in the \ndata\n folder with the server.\n\n\nNotebooks are for exploration and communication\n\n\nNotebook packages like the \nJupyter notebook\n, \nBeaker notebook\n, \nZeppelin\n, and other literate programming tools are very effective for exploratory data analysis. However, these tools can be less effective for reproducing an analysis. When we use notebooks in our work, we often subdivide the \nnotebooks\n folder. For example, \nnotebooks/exploratory\n contains initial explorations, whereas \nnotebooks/reports\n is more polished work that can be exported as html to the \nreports\n directory.\n\n\nSince notebooks are challenging objects for source control (e.g., diffs of the \njson\n are often not human-readable and merging is near impossible), we recommended not collaborating directly with others on Jupyter notebooks. There are two steps we recommend for using notebooks effectively:\n\n\n\n\n\n\nFollow a naming convention that shows the owner and the order the analysis was done in. We use the format \n<step>-<ghuser>-<description>.ipynb\n (e.g., \n0.3-bull-visualize-distributions.ipynb\n).\n\n\n\n\n\n\nRefactor the good parts. Don't write code to do the same task in multiple notebooks. If it's a data preprocessing task, put it in the pipeline at \nsrc/data/make_dataset.py\n and load data from \ndata/interim\n. If it's useful utility code, refactor it to \nsrc\n.\n\n\n\n\n\n\nNow by default we turn the project into a Python package (see the \nsetup.py\n file). You can import your code and use it in notebooks with a cell like the following:\n\n\n# OPTIONAL: Load the \"autoreload\" extension so that code can change\n%load_ext autoreload\n\n# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n%autoreload 2\n\nfrom src.data import make_dataset\n\n\n\n\nAnalysis is a DAG\n\n\nOften in an analysis you have long-running steps that preprocess data or train models. If these steps have been run already (and you have stored the output somewhere like the \ndata/interim\n directory), you don't want to wait to rerun them every time. We prefer \nmake\n for managing steps that depend on each other, especially the long-running ones. Make is a common tool on Unix-based platforms (and \nis available for Windows\n). Following the \nmake\n documentation\n, \nMakefile conventions\n, and \nportability guide\n will help ensure your Makefiles work effectively across systems. Here are \nsome\n \nexamples\n to \nget started\n. A number of data folks use \nmake\n as their tool of choice, including \nMike Bostock\n.\n\n\nThere are other tools for managing DAGs that are written in Python instead of a DSL (e.g., \nPaver\n, \nLuigi\n, \nAirflow\n, \nSnakemake\n, \nRuffus\n, or \nJoblib\n). Feel free to use these if they are more appropriate for your analysis.\n\n\nBuild from the environment up\n\n\nThe first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together.\n\n\nOne effective approach to this is use \nvirtualenv\n (we recommend \nvirtualenvwrapper\n for managing virtualenvs). By listing all of your requirements in the repository (we include a \nrequirements.txt\n file) you can easily track the packages needed to recreate the analysis. Here is a good workflow:\n\n\n\n\nRun \nmkvirtualenv\n when creating a new project\n\n\npip install\n the packages that your analysis needs\n\n\nRun \npip freeze > requirements.txt\n to pin the exact package versions used to recreate the analysis\n\n\nIf you find you need to install another package, run \npip freeze > requirements.txt\n again and commit the changes to version control.\n\n\n\n\nIf you have more complex requirements for recreating your environment, consider a virtual machine based approach such as \nDocker\n or \nVagrant\n. Both of these tools use text-based formats (Dockerfile and Vagrantfile respectively) you can easily add to source control to describe how to create a virtual machine with the requirements you need.\n\n\nKeep secrets and configuration out of version control\n\n\nYou \nreally\n don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the \nTwelve Factor App\n principles on this point. Here's one way to do this:\n\n\nStore your secrets and config variables in a special file\n\n\nCreate a \n.env\n file in the project root folder. Thanks to the \n.gitignore\n, this file should never get committed into the version control repository. Here's an example:\n\n\n# example .env file\nDATABASE_URL=postgres://username:password@localhost:5432/dbname\nAWS_ACCESS_KEY=myaccesskey\nAWS_SECRET_ACCESS_KEY=mysecretkey\nOTHER_VARIABLE=something\n\n\n\n\nUse a package to load these variables automatically.\n\n\nIf you look at the stub script in \nsrc/data/make_dataset.py\n, it uses a package called \npython-dotenv\n to load up all the entries in this file as environment variables so they are accessible with \nos.environ.get\n. Here's an example snippet adapted from the \npython-dotenv\n documentation:\n\n\n# src/data/dotenv_example.py\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\n# find .env automagically by walking up directories until it's found\ndotenv_path = find_dotenv()\n\n# load up the entries as environment variables\nload_dotenv(dotenv_path)\n\ndatabase_url = os.environ.get(\"DATABASE_URL\")\nother_variable = os.environ.get(\"OTHER_VARIABLE\")\n\n\n\n\nAWS CLI configuration\n\n\nWhen using Amazon S3 to store data, a simple method of managing AWS access is to set your access keys to environment variables. However, managing mutiple sets of keys on a single machine (e.g. when working on multiple projects) it is best to use a \ncredentials file\n, typically located in \n~/.aws/credentials\n. A typical file might look like:\n\n\n[default]\naws_access_key_id=myaccesskey\naws_secret_access_key=mysecretkey\n\n[another_project]\naws_access_key_id=myprojectaccesskey\naws_secret_access_key=myprojectsecretkey\n\n\n\n\nYou can add the profile name when initialising a project; assuming no applicable environment variables are set, the profile credentials will be used be default.\n\n\nBe conservative in changing the default folder structure\n\n\nTo keep this structure broadly applicable for many different kinds of projects, we think the best approach is to be liberal in changing the folders around for \nyour\n project, but be conservative in changing the default structure for \nall\n projects.\n\n\nWe've created a \nfolder-layout\n label specifically for issues proposing to add, subtract, rename, or move folders around. More generally, we've also created a \nneeds-discussion\n label for issues that should have some careful discussion and broad support before being implemented.\n\n\nContributing\n\n\nThe Cookiecutter Data Science project is opinionated, but not afraid to be wrong. Best practices change, tools evolve, and lessons are learned. \nThe goal of this project is to make it easier to start, structure, and share an analysis.\n \nPull requests\n and \nfiling issues\n is encouraged. We'd love to hear what works for you, and what doesn't.\n\n\nIf you use the Cookiecutter Data Science project, link back to this page or \ngive us a holler\n and \nlet us know\n!\n\n\nLinks to related projects and references\n\n\nProject structure and reproducibility is talked about more in the R research community. Here are some projects and blog posts if you're working in R that may help you out.\n\n\n\n\nProject Template\n - An R data analysis template\n\n\n\"\nDesigning projects\n\" on Nice R Code\n\n\n\"\nMy research workflow\n\" on Carlboettifer.info\n\n\n\"\nA Quick Guide to Organizing Computational Biology Projects\n\" in PLOS Computational Biology\n\n\n\n\nFinally, a huge thanks to the \nCookiecutter\n project (\ngithub\n), which is helping us all spend less time thinking about and writing boilerplate and more time getting things done.",
            "title": "Home"
        },
        {
            "location": "/#cookiecutter-data-science",
            "text": "A logical, reasonably standardized, but flexible project structure for doing and sharing data science work.",
            "title": "Cookiecutter Data Science"
        },
        {
            "location": "/#why-use-this-project-structure",
            "text": "We're not talking about bikeshedding the indentation aesthetics or pedantic formatting standards \u2014 ultimately, data science code quality is about correctness and reproducibility.   When we think about data analysis, we often think just about the resulting reports, insights, or visualizations. While these end products are generally the main event, it's easy to focus on making the products  look nice  and ignore the  quality of the code that generates them . Because these end products are created programmatically,  code quality is still important ! And we're not talking about bikeshedding the indentation aesthetics or pedantic formatting standards \u2014 ultimately, data science code quality is about correctness and reproducibility.  It's no secret that good analyses are often the result of very scattershot and serendipitous explorations. Tentative experiments and rapidly testing approaches that might not work out are all part of the process for getting to the good stuff, and there is no magic bullet to turn data exploration into a simple, linear progression.  That being said, once started it is not a process that lends itself to thinking carefully about the structure of your code or project layout, so it's best to start with a clean, logical structure and stick to it throughout. We think it's a pretty big win all around to use a fairly standardized setup like this one. Here's why:",
            "title": "Why use this project structure?"
        },
        {
            "location": "/#other-people-will-thank-you",
            "text": "Nobody sits around before creating a new Rails project to figure out where they want to put their views; they just run  rails new  to get a standard project skeleton like everybody else.   A well-defined, standard project structure means that a newcomer can begin to understand an analysis without digging in to extensive documentation. It also means that they don't necessarily have to read 100% of the code before knowing where to look for very specific things.  Well organized code tends to be self-documenting in that the organization itself provides context for your code without much overhead. People will thank you for this because they can:   Collaborate more easily with you on this analysis  Learn from your analysis about the process and the domain  Feel confident in the conclusions at which the analysis arrives   A good example of this can be found in any of the major web development frameworks like Django or Ruby on Rails. Nobody sits around before creating a new Rails project to figure out where they want to put their views; they just run  rails new  to get a standard project skeleton like everybody else. Because that default project structure is  logical  and  reasonably standard across most projects , it is much easier for somebody who has never seen a particular project to figure out where they would find the various moving parts.  Another great example is the  Filesystem Hierarchy Standard  for Unix-like systems. The  /etc  directory has a very specific purpose, as does the  /tmp  folder, and everybody (more or less) agrees to honor that social contract. That means a Red Hat user and an Ubuntu user both know roughly where to look for certain types of files, even when using each other's system \u2014 or any other standards-compliant system for that matter!  Ideally, that's how it should be when a colleague opens up your data science project.",
            "title": "Other people will thank you"
        },
        {
            "location": "/#you-will-thank-you",
            "text": "Ever tried to reproduce an analysis that you did a few months ago or even a few years ago? You may have written the code, but it's now impossible to decipher whether you should use  make_figures.py.old ,  make_figures_working.py  or  new_make_figures01.py  to get things done. Here are some questions we've learned to ask with a sense of existential dread:   Are we supposed to go in and join the column X to the data before we get started or did that come from one of the notebooks?  Come to think of it, which notebook do we have to run first before running the plotting code: was it \"process data\" or \"clean data\"?  Where did the shapefiles get downloaded from for the geographic plots?  Et cetera, times infinity.   These types of questions are painful and are symptoms of a disorganized project. A good project structure encourages practices that make it easier to come back to old work, for example separation of concerns, abstracting analysis as a  DAG , and engineering best practices like version control.",
            "title": "You will thank you"
        },
        {
            "location": "/#nothing-here-is-binding",
            "text": "\"A foolish consistency is the hobgoblin of little minds\" \u2014 Ralph Waldo Emerson (and  PEP 8! )   Disagree with a couple of the default folder names? Working on a project that's a little nonstandard and doesn't exactly fit with the current structure? Prefer to use a different package than one of the (few) defaults?  Go for it!  This is a lightweight structure, and is intended to be a good  starting point  for many projects. Or, as PEP 8 put it:   Consistency within a project is more important. Consistency within one module or function is the most important. ... However, know when to be inconsistent -- sometimes style guide recommendations just aren't applicable. When in doubt, use your best judgment. Look at other examples and decide what looks best. And don't hesitate to ask!",
            "title": "Nothing here is binding"
        },
        {
            "location": "/#getting-started",
            "text": "With this in mind, we've created a data science cookiecutter template for projects in Python. Your analysis doesn't have to be in Python, but the template does provide some Python boilerplate that you'd want to remove (in the  src  folder for example, and the Sphinx documentation skeleton in  docs ).",
            "title": "Getting started"
        },
        {
            "location": "/#requirements",
            "text": "Python 2.7 or 3.5  cookiecutter Python package  >= 1.4.0:  pip install cookiecutter",
            "title": "Requirements"
        },
        {
            "location": "/#starting-a-new-project",
            "text": "Starting a new project is as easy as running this command at the command line. No need to create a directory first, the cookiecutter will do it for you.  cookiecutter https://github.com/drivendata/cookiecutter-data-science",
            "title": "Starting a new project"
        },
        {
            "location": "/#example",
            "text": "",
            "title": "Example"
        },
        {
            "location": "/#directory-structure",
            "text": "\u251c\u2500\u2500 LICENSE\n\u251c\u2500\u2500 Makefile           <- Makefile with commands like `make data` or `make train`\n\u251c\u2500\u2500 README.md          <- The top-level README for developers using this project.\n\u251c\u2500\u2500 data\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 external       <- Data from third party sources.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 interim        <- Intermediate data that has been transformed.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 processed      <- The final, canonical data sets for modeling.\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 raw            <- The original, immutable data dump.\n\u2502\n\u251c\u2500\u2500 docs               <- A default Sphinx project; see sphinx-doc.org for details\n\u2502\n\u251c\u2500\u2500 models             <- Trained and serialized models, model predictions, or model summaries\n\u2502\n\u251c\u2500\u2500 notebooks          <- Jupyter notebooks. Naming convention is a number (for ordering),\n\u2502                         the creator's initials, and a short `-` delimited description, e.g.\n\u2502                         `1.0-jqp-initial-data-exploration`.\n\u2502\n\u251c\u2500\u2500 references         <- Data dictionaries, manuals, and all other explanatory materials.\n\u2502\n\u251c\u2500\u2500 reports            <- Generated analysis as HTML, PDF, LaTeX, etc.\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 figures        <- Generated graphics and figures to be used in reporting\n\u2502\n\u251c\u2500\u2500 requirements.txt   <- The requirements file for reproducing the analysis environment, e.g.\n\u2502                         generated with `pip freeze > requirements.txt`\n\u2502\n\u251c\u2500\u2500 setup.py           <- Make this project pip installable with `pip install -e`\n\u251c\u2500\u2500 src                <- Source code for use in this project.\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 __init__.py    <- Makes src a Python module\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 data           <- Scripts to download or generate data\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 make_dataset.py\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 features       <- Scripts to turn raw data into features for modeling\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 build_features.py\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 models         <- Scripts to train models and then use trained models to make\n\u2502   \u2502   \u2502                 predictions\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u251c\u2500\u2500 predict_model.py\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_model.py\n\u2502   \u2502\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 visualization  <- Scripts to create exploratory and results oriented visualizations\n\u2502\u00a0\u00a0     \u2514\u2500\u2500 visualize.py\n\u2502\n\u2514\u2500\u2500 tox.ini            <- tox file with settings for running tox; see tox.testrun.org",
            "title": "Directory structure"
        },
        {
            "location": "/#opinions",
            "text": "There are some opinions implicit in the project structure that have grown out of our experience with what works and what doesn't when collaborating on data science projects. Some of the opinions are about workflows, and some of the opinions are about tools that make life easier. Here are some of the beliefs which this project is built on\u2014if you've got thoughts, please  contribute or share them .",
            "title": "Opinions"
        },
        {
            "location": "/#data-is-immutable",
            "text": "Don't ever edit your raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable. The code you write should move the raw data through a pipeline to your final analysis. You shouldn't have to run all of the steps every time you want to make a new figure (see  Analysis is a DAG ), but anyone should be able to reproduce the final products with only the code in  src  and the data in  data/raw .  Also, if data is immutable, it doesn't need source control in the same way that code does. Therefore,  by default, the data folder is included in the  .gitignore  file.  If you have a small amount of data that rarely changes, you may want to include the data in the repository. Github currently warns if files are over 50MB and rejects files over 100MB. Some other options for storing/syncing large data include  AWS S3  with a syncing tool (e.g.,  s3cmd ),  Git Large File Storage ,  Git Annex , and  dat . Currently by default, we ask for an S3 bucket and use  AWS CLI  to sync data in the  data  folder with the server.",
            "title": "Data is immutable"
        },
        {
            "location": "/#notebooks-are-for-exploration-and-communication",
            "text": "Notebook packages like the  Jupyter notebook ,  Beaker notebook ,  Zeppelin , and other literate programming tools are very effective for exploratory data analysis. However, these tools can be less effective for reproducing an analysis. When we use notebooks in our work, we often subdivide the  notebooks  folder. For example,  notebooks/exploratory  contains initial explorations, whereas  notebooks/reports  is more polished work that can be exported as html to the  reports  directory.  Since notebooks are challenging objects for source control (e.g., diffs of the  json  are often not human-readable and merging is near impossible), we recommended not collaborating directly with others on Jupyter notebooks. There are two steps we recommend for using notebooks effectively:    Follow a naming convention that shows the owner and the order the analysis was done in. We use the format  <step>-<ghuser>-<description>.ipynb  (e.g.,  0.3-bull-visualize-distributions.ipynb ).    Refactor the good parts. Don't write code to do the same task in multiple notebooks. If it's a data preprocessing task, put it in the pipeline at  src/data/make_dataset.py  and load data from  data/interim . If it's useful utility code, refactor it to  src .    Now by default we turn the project into a Python package (see the  setup.py  file). You can import your code and use it in notebooks with a cell like the following:  # OPTIONAL: Load the \"autoreload\" extension so that code can change\n%load_ext autoreload\n\n# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n%autoreload 2\n\nfrom src.data import make_dataset",
            "title": "Notebooks are for exploration and communication"
        },
        {
            "location": "/#analysis-is-a-dag",
            "text": "Often in an analysis you have long-running steps that preprocess data or train models. If these steps have been run already (and you have stored the output somewhere like the  data/interim  directory), you don't want to wait to rerun them every time. We prefer  make  for managing steps that depend on each other, especially the long-running ones. Make is a common tool on Unix-based platforms (and  is available for Windows ). Following the  make  documentation ,  Makefile conventions , and  portability guide  will help ensure your Makefiles work effectively across systems. Here are  some   examples  to  get started . A number of data folks use  make  as their tool of choice, including  Mike Bostock .  There are other tools for managing DAGs that are written in Python instead of a DSL (e.g.,  Paver ,  Luigi ,  Airflow ,  Snakemake ,  Ruffus , or  Joblib ). Feel free to use these if they are more appropriate for your analysis.",
            "title": "Analysis is a DAG"
        },
        {
            "location": "/#build-from-the-environment-up",
            "text": "The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together.  One effective approach to this is use  virtualenv  (we recommend  virtualenvwrapper  for managing virtualenvs). By listing all of your requirements in the repository (we include a  requirements.txt  file) you can easily track the packages needed to recreate the analysis. Here is a good workflow:   Run  mkvirtualenv  when creating a new project  pip install  the packages that your analysis needs  Run  pip freeze > requirements.txt  to pin the exact package versions used to recreate the analysis  If you find you need to install another package, run  pip freeze > requirements.txt  again and commit the changes to version control.   If you have more complex requirements for recreating your environment, consider a virtual machine based approach such as  Docker  or  Vagrant . Both of these tools use text-based formats (Dockerfile and Vagrantfile respectively) you can easily add to source control to describe how to create a virtual machine with the requirements you need.",
            "title": "Build from the environment up"
        },
        {
            "location": "/#keep-secrets-and-configuration-out-of-version-control",
            "text": "You  really  don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the  Twelve Factor App  principles on this point. Here's one way to do this:",
            "title": "Keep secrets and configuration out of version control"
        },
        {
            "location": "/#store-your-secrets-and-config-variables-in-a-special-file",
            "text": "Create a  .env  file in the project root folder. Thanks to the  .gitignore , this file should never get committed into the version control repository. Here's an example:  # example .env file\nDATABASE_URL=postgres://username:password@localhost:5432/dbname\nAWS_ACCESS_KEY=myaccesskey\nAWS_SECRET_ACCESS_KEY=mysecretkey\nOTHER_VARIABLE=something",
            "title": "Store your secrets and config variables in a special file"
        },
        {
            "location": "/#use-a-package-to-load-these-variables-automatically",
            "text": "If you look at the stub script in  src/data/make_dataset.py , it uses a package called  python-dotenv  to load up all the entries in this file as environment variables so they are accessible with  os.environ.get . Here's an example snippet adapted from the  python-dotenv  documentation:  # src/data/dotenv_example.py\nimport os\nfrom dotenv import load_dotenv, find_dotenv\n\n# find .env automagically by walking up directories until it's found\ndotenv_path = find_dotenv()\n\n# load up the entries as environment variables\nload_dotenv(dotenv_path)\n\ndatabase_url = os.environ.get(\"DATABASE_URL\")\nother_variable = os.environ.get(\"OTHER_VARIABLE\")",
            "title": "Use a package to load these variables automatically."
        },
        {
            "location": "/#aws-cli-configuration",
            "text": "When using Amazon S3 to store data, a simple method of managing AWS access is to set your access keys to environment variables. However, managing mutiple sets of keys on a single machine (e.g. when working on multiple projects) it is best to use a  credentials file , typically located in  ~/.aws/credentials . A typical file might look like:  [default]\naws_access_key_id=myaccesskey\naws_secret_access_key=mysecretkey\n\n[another_project]\naws_access_key_id=myprojectaccesskey\naws_secret_access_key=myprojectsecretkey  You can add the profile name when initialising a project; assuming no applicable environment variables are set, the profile credentials will be used be default.",
            "title": "AWS CLI configuration"
        },
        {
            "location": "/#be-conservative-in-changing-the-default-folder-structure",
            "text": "To keep this structure broadly applicable for many different kinds of projects, we think the best approach is to be liberal in changing the folders around for  your  project, but be conservative in changing the default structure for  all  projects.  We've created a  folder-layout  label specifically for issues proposing to add, subtract, rename, or move folders around. More generally, we've also created a  needs-discussion  label for issues that should have some careful discussion and broad support before being implemented.",
            "title": "Be conservative in changing the default folder structure"
        },
        {
            "location": "/#contributing",
            "text": "The Cookiecutter Data Science project is opinionated, but not afraid to be wrong. Best practices change, tools evolve, and lessons are learned.  The goal of this project is to make it easier to start, structure, and share an analysis.   Pull requests  and  filing issues  is encouraged. We'd love to hear what works for you, and what doesn't.  If you use the Cookiecutter Data Science project, link back to this page or  give us a holler  and  let us know !",
            "title": "Contributing"
        },
        {
            "location": "/#links-to-related-projects-and-references",
            "text": "Project structure and reproducibility is talked about more in the R research community. Here are some projects and blog posts if you're working in R that may help you out.   Project Template  - An R data analysis template  \" Designing projects \" on Nice R Code  \" My research workflow \" on Carlboettifer.info  \" A Quick Guide to Organizing Computational Biology Projects \" in PLOS Computational Biology   Finally, a huge thanks to the  Cookiecutter  project ( github ), which is helping us all spend less time thinking about and writing boilerplate and more time getting things done.",
            "title": "Links to related projects and references"
        }
    ]
}